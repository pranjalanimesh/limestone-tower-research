{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "stocks = pd.read_csv('../data_challenge_stock_prices.csv')\n",
    "stocks.head()\n",
    "indices = pd.read_csv('../data_challenge_index_prices.csv')\n",
    "indices.head()\n",
    "print(len(stocks))\n",
    "print(len(indices))\n",
    "stock_returns = stocks.pct_change().dropna()*10000\n",
    "index_returns = indices.pct_change().dropna() * 10000\n",
    "stock_returns_T =stock_returns.T\n",
    "index_returns_T =index_returns.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # find the mean of each column/feature\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_stock_returns, X_mu, X_sigma = zscore_normalize_features(stock_returns)\n",
    "norm_index_returns, X_mu, X_sigma = zscore_normalize_features(index_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:15<00:00,  3.95s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sum_of_squared_distances = []\n",
    "K = range(1,20)\n",
    "\n",
    "for num_clusters in tqdm(K) :\n",
    " kmeans = KMeans(n_clusters=num_clusters)\n",
    " kmeans.fit(norm_stock_returns.T)\n",
    " Sum_of_squared_distances.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(K,Sum_of_squared_distances,'bx-')\n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('Sum of squared distances/Inertia') \n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 2 0 1 2 2 3 2 0 0 3 1 0 1 2 2 0 2 0 0 0 2 1 0 0 3 2 3 3 1 2 2 1 1 3\n",
      " 1 0 0 3 3 2 0 3 0 3 2 1 0 3 2 3 2 0 3 1 3 2 3 2 1 1 2 1 1 2 0 1 3 1 3 2 3\n",
      " 3 0 1 3 0 0 3 2 0 1 3 1 1 2 0 1 0 2 3 1 1 1 3 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "norm_stock_returns_T = norm_stock_returns.T\n",
    "M=4\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(norm_stock_returns_T)\n",
    "clusters = kmeans.labels_\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = {i: [] for i in range(M)}\n",
    "for stock, sector in zip(norm_stock_returns.columns, clusters):\n",
    "    sectors[sector].append(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n",
      "25\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sectors)):\n",
    "    print(len(sectors[i]))\n",
    "\n",
    "# print(4*2**25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0 belongs to sector 0 with correlation 0.4491892421918561\n",
      "Index 1 belongs to sector 1 with correlation 0.41689679891886233\n",
      "Index 2 belongs to sector 0 with correlation 0.6080348388645738\n",
      "Index 3 belongs to sector 3 with correlation 0.4493291855252194\n",
      "Index 4 belongs to sector 2 with correlation 0.45118098353857844\n",
      "Index 5 belongs to sector 1 with correlation 0.5930012430217189\n",
      "Index 6 belongs to sector 3 with correlation 0.4483911874780483\n",
      "Index 7 could not be solved with max correlation 0.39900047783350356\n",
      "Index 8 belongs to sector 3 with correlation 0.4065356104361991\n",
      "Index 9 belongs to sector 2 with correlation 0.4505966571014396\n",
      "Index 10 belongs to sector 1 with correlation 0.44898020711260517\n",
      "Index 11 belongs to sector 0 with correlation 0.4474516716986008\n",
      "Index 12 belongs to sector 2 with correlation 0.44683401619679664\n",
      "Index 13 belongs to sector 3 with correlation 0.4501566351701191\n",
      "Index 14 belongs to sector 2 with correlation 0.5969951378854911\n"
     ]
    }
   ],
   "source": [
    "# MLP regressor\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Create an instance of the MLPRegressor class\n",
    "# model = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', alpha=0.01, max_iter=200)\n",
    "\n",
    "# Train the model on the training data\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute the mean absolute error\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# print(\"Mean Absolute Error: \", mae)\n",
    "\n",
    "index_predictions = pd.DataFrame()\n",
    "\n",
    "for index in index_returns.columns:\n",
    "    \n",
    "    best_score = -1\n",
    "    best_sector = None\n",
    "    best_preds = None\n",
    "    \n",
    "    for sector in sectors:\n",
    "        X = stock_returns[sectors[sector]]\n",
    "        y = index_returns[index]\n",
    "        # Scale the input features\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X)\n",
    "\n",
    "        # print(type(y))\n",
    "\n",
    "        # Create an instance of the MLPRegressor class\n",
    "        model = MLPRegressor(hidden_layer_sizes=(15,5), activation='relu', solver='adam', alpha=0.01, max_iter=500)\n",
    "\n",
    "        # Train the model on the training data\n",
    "        model.fit(X_train, y)\n",
    "\n",
    "        # Make predictions\n",
    "        preds = model.predict(X_train)\n",
    "\n",
    "        score = np.corrcoef(y,preds)[0,1]\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_sector = sector\n",
    "            best_preds = preds\n",
    "            # print(score)\n",
    "\n",
    "    if best_score >= 0.4:  # 40% predictive correlation\n",
    "        print(f\"Index {index} belongs to sector {best_sector} with correlation {best_score}\")\n",
    "        index_predictions[index] = best_preds\n",
    "    else:\n",
    "        print(f\"Index {index} could not be solved with max correlation {best_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6  \\\n",
      "0   1.552294  0.053637 -0.198452 -0.146502  0.312102 -0.042221  0.289724   \n",
      "1   0.053637  0.112732 -0.006436 -0.030563  0.056624 -0.037005  0.060395   \n",
      "2  -0.198452 -0.006436  0.447218  0.015333 -0.036658  0.005984 -0.028497   \n",
      "3  -0.146502 -0.030563  0.015333  2.390012 -0.125128  0.023146 -0.691857   \n",
      "4   0.312102  0.056624 -0.036658 -0.125128  1.629259 -0.046170  0.268379   \n",
      "5  -0.042221 -0.037005  0.005984  0.023146 -0.046170  0.444953 -0.046470   \n",
      "6   0.289724  0.060395 -0.028497 -0.691857  0.268379 -0.046470  1.510790   \n",
      "8   0.054364  0.011745 -0.004950 -0.139656  0.051481 -0.009212  0.287305   \n",
      "9  -0.177693 -0.038730  0.018408  0.082691 -0.773980  0.030717 -0.164231   \n",
      "10 -0.093503 -0.153240  0.010231  0.071949 -0.100323  0.099966 -0.131304   \n",
      "11  0.076545  0.003580  0.101679 -0.021106  0.021292  0.002519  0.016412   \n",
      "12  0.005119  0.000718  0.001901 -0.002908  0.009571  0.001627  0.003274   \n",
      "13 -0.000995 -0.000616  0.000722 -0.290494 -0.001087  0.007141 -0.013862   \n",
      "14 -0.032742 -0.006115  0.003762  0.011489 -0.195618  0.006403 -0.030823   \n",
      "\n",
      "           8         9        10        11        12        13        14  \n",
      "0   0.054364 -0.177693 -0.093503  0.076545  0.005119 -0.000995 -0.032742  \n",
      "1   0.011745 -0.038730 -0.153240  0.003580  0.000718 -0.000616 -0.006115  \n",
      "2  -0.004950  0.018408  0.010231  0.101679  0.001901  0.000722  0.003762  \n",
      "3  -0.139656  0.082691  0.071949 -0.021106 -0.002908 -0.290494  0.011489  \n",
      "4   0.051481 -0.773980 -0.100323  0.021292  0.009571 -0.001087 -0.195618  \n",
      "5  -0.009212  0.030717  0.099966  0.002519  0.001627  0.007141  0.006403  \n",
      "6   0.287305 -0.164231 -0.131304  0.016412  0.003274 -0.013862 -0.030823  \n",
      "8   0.096207 -0.032409 -0.026817  0.002900  0.000078 -0.001010 -0.006229  \n",
      "9  -0.032409  2.551283  0.091540 -0.029515 -0.377615 -0.008226  0.085027  \n",
      "10 -0.026817  0.091540  2.643403 -0.008680 -0.006591 -0.006606  0.007068  \n",
      "11  0.002900 -0.029515 -0.008680  0.751153  0.023551  0.017148  0.001574  \n",
      "12  0.000078 -0.377615 -0.006591  0.023551  0.718231  0.013733  0.124197  \n",
      "13 -0.001010 -0.008226 -0.006606  0.017148  0.013733  0.693429  0.003064  \n",
      "14 -0.006229  0.085027  0.007068  0.001574  0.124197  0.003064  0.439724  \n"
     ]
    }
   ],
   "source": [
    "covariance_matrix = index_predictions.cov()\n",
    "print(covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6  \\\n",
      "0   0.216988  0.024321 -0.067169 -0.017896  0.039923 -0.012740  0.038910   \n",
      "1   0.024321  0.168380 -0.006933 -0.011801  0.023328 -0.036732  0.026167   \n",
      "2  -0.067169 -0.006933  0.389192  0.004665 -0.011477  0.003778 -0.008204   \n",
      "3  -0.017896 -0.011801  0.004665  0.215825 -0.013842  0.005821 -0.081416   \n",
      "4   0.039923  0.023328 -0.011477 -0.013842  0.189640 -0.012581  0.032873   \n",
      "5  -0.012740 -0.036732  0.003778  0.005821 -0.012581  0.335826 -0.013573   \n",
      "6   0.038910  0.026167 -0.008204 -0.081416  0.032873 -0.013573  0.193811   \n",
      "7   0.147830  0.016884 -0.037194 -0.011631  0.028083 -0.008391  0.027539   \n",
      "9  -0.016886 -0.012190  0.003984  0.006838 -0.065192  0.006508 -0.015182   \n",
      "10 -0.009026 -0.048886  0.002099  0.006098 -0.008873  0.022548 -0.012333   \n",
      "11  0.006513  0.000610  0.057052 -0.002418  0.001480  0.001381  0.000666   \n",
      "12  0.001302  0.000667  0.000993 -0.000688  0.000482  0.000500  0.000927   \n",
      "13  0.003200  0.001658 -0.000287 -0.054742  0.002682  0.001990  0.015852   \n",
      "14 -0.012687 -0.007322  0.003934  0.003647 -0.066017  0.004846 -0.010866   \n",
      "\n",
      "           7         9        10        11        12        13        14  \n",
      "0   0.147830 -0.016886 -0.009026  0.006513  0.001302  0.003200 -0.012687  \n",
      "1   0.016884 -0.012190 -0.048886  0.000610  0.000667  0.001658 -0.007322  \n",
      "2  -0.037194  0.003984  0.002099  0.057052  0.000993 -0.000287  0.003934  \n",
      "3  -0.011631  0.006838  0.006098 -0.002418 -0.000688 -0.054742  0.003647  \n",
      "4   0.028083 -0.065192 -0.008873  0.001480  0.000482  0.002682 -0.066017  \n",
      "5  -0.008391  0.006508  0.022548  0.001381  0.000500  0.001990  0.004846  \n",
      "6   0.027539 -0.015182 -0.012333  0.000666  0.000927  0.015852 -0.010866  \n",
      "7   0.175223 -0.012043 -0.006689  0.004093  0.000959  0.002135 -0.008424  \n",
      "9  -0.012043  0.194913  0.006534 -0.002964 -0.050883 -0.002358  0.020395  \n",
      "10 -0.006689  0.006534  0.206637 -0.000620 -0.001188 -0.001982  0.001666  \n",
      "11  0.004093 -0.002964 -0.000620  0.196165  0.006406  0.004668  0.001203  \n",
      "12  0.000959 -0.050883 -0.001188  0.006406  0.204155  0.003829  0.059371  \n",
      "13  0.002135 -0.002358 -0.001982  0.004668  0.003829  0.188497  0.000263  \n",
      "14 -0.008424  0.020395  0.001666  0.001203  0.059371  0.000263  0.359969  \n"
     ]
    }
   ],
   "source": [
    "covariance_matrix = index_predictions.cov()\n",
    "print(covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.55229386 0.11273153 0.44721776 2.39001198 1.62925947 0.44495321\n",
      " 1.51079023 0.09620662 2.55128262 2.6434034  0.75115275 0.71823061\n",
      " 0.69342853 0.43972415]\n",
      "           0\n",
      "0   1.552294\n",
      "1   0.112732\n",
      "2   0.447218\n",
      "3   2.390012\n",
      "4   1.629259\n",
      "5   0.444953\n",
      "6   1.510790\n",
      "8   0.096207\n",
      "9   2.551283\n",
      "10  2.643403\n",
      "11  0.751153\n",
      "12  0.718231\n",
      "13  0.693429\n",
      "14  0.439724\n",
      "RangeIndex(start=0, stop=1, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(np.diag(covariance_matrix))\n",
    "variances = pd.DataFrame(np.diag(covariance_matrix),index=index_predictions.columns)\n",
    "print(variances)\n",
    "print(variances.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21698843 0.16838004 0.38919181 0.21582513 0.18964034 0.33582588\n",
      " 0.19381133 0.17522328 0.19491299 0.20663696 0.19616497 0.20415521\n",
      " 0.18849696 0.35996872]\n",
      "           0\n",
      "0   0.216988\n",
      "1   0.168380\n",
      "2   0.389192\n",
      "3   0.215825\n",
      "4   0.189640\n",
      "5   0.335826\n",
      "6   0.193811\n",
      "7   0.175223\n",
      "9   0.194913\n",
      "10  0.206637\n",
      "11  0.196165\n",
      "12  0.204155\n",
      "13  0.188497\n",
      "14  0.359969\n",
      "RangeIndex(start=0, stop=1, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(np.diag(covariance_matrix))\n",
    "variances = pd.DataFrame(np.diag(covariance_matrix),index=index_predictions.columns)\n",
    "print(variances)\n",
    "print(variances.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (14,) and (15,) not aligned: 14 (dim 0) != 15 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ROG\\Desktop\\Limestone Data comp\\limestone-tower-research\\other.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X25sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m bounds \u001b[39m=\u001b[39m [(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m14\u001b[39m)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X25sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Solve for the optimal portfolio allocation\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X25sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m result \u001b[39m=\u001b[39m minimize(objective, initial_weights, args\u001b[39m=\u001b[39;49m(predicted_returns, variances), constraints\u001b[39m=\u001b[39;49mconstraints, bounds\u001b[39m=\u001b[39;49mbounds)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X25sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m weights \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mx\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X25sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Compute the allocation for each stock\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ROG\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:701\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    698\u001b[0m     res \u001b[39m=\u001b[39m _minimize_cobyla(fun, x0, args, constraints, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    699\u001b[0m                             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    700\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mslsqp\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 701\u001b[0m     res \u001b[39m=\u001b[39m _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0;32m    702\u001b[0m                           constraints, callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    703\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrust-constr\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    704\u001b[0m     res \u001b[39m=\u001b[39m _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[0;32m    705\u001b[0m                                        bounds, constraints,\n\u001b[0;32m    706\u001b[0m                                        callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\ROG\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_slsqp_py.py:374\u001b[0m, in \u001b[0;36m_minimize_slsqp\u001b[1;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    371\u001b[0m     xu[infbnd[:, \u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n\u001b[0;32m    373\u001b[0m \u001b[39m# ScalarFunction provides function and gradient evaluation\u001b[39;00m\n\u001b[1;32m--> 374\u001b[0m sf \u001b[39m=\u001b[39m _prepare_scalar_function(func, x, jac\u001b[39m=\u001b[39;49mjac, args\u001b[39m=\u001b[39;49margs, epsilon\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    375\u001b[0m                               finite_diff_rel_step\u001b[39m=\u001b[39;49mfinite_diff_rel_step,\n\u001b[0;32m    376\u001b[0m                               bounds\u001b[39m=\u001b[39;49mnew_bounds)\n\u001b[0;32m    377\u001b[0m \u001b[39m# gh11403 SLSQP sometimes exceeds bounds by 1 or 2 ULP, make sure this\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[39m# doesn't get sent to the func/grad evaluator.\u001b[39;00m\n\u001b[0;32m    379\u001b[0m wrapped_fun \u001b[39m=\u001b[39m _clip_x_for_func(sf\u001b[39m.\u001b[39mfun, new_bounds)\n",
      "File \u001b[1;32mc:\\Users\\ROG\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_optimize.py:263\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    259\u001b[0m     bounds \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf)\n\u001b[0;32m    261\u001b[0m \u001b[39m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m sf \u001b[39m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m    264\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[39m=\u001b[39;49mepsilon)\n\u001b[0;32m    266\u001b[0m \u001b[39mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mc:\\Users\\ROG\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl \u001b[39m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    160\u001b[0m \u001b[39m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mif\u001b[39;00m callable(grad):\n",
      "File \u001b[1;32mc:\\Users\\ROG\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ROG\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32mc:\\Users\\ROG\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "\u001b[1;32mc:\\Users\\ROG\\Desktop\\Limestone Data comp\\limestone-tower-research\\other.ipynb Cell 14\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(weights, predicted_returns, variances)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjective\u001b[39m(weights, predicted_returns, variances):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m    Compute the objective function for mean-variance optimization.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     mean_return \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(weights\u001b[39m.\u001b[39;49mT, predicted_returns\u001b[39m.\u001b[39;49mmean(axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mprint\u001b[39m(mean_return\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     variance \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(weights\u001b[39m.\u001b[39mT, np\u001b[39m.\u001b[39mdot(np\u001b[39m.\u001b[39mdiag(variances), weights))\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (14,) and (15,) not aligned: 14 (dim 0) != 15 (dim 0)"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Extract the actual stock prices and predicted stock returns for each stock\n",
    "\n",
    "index_list = list(range(15))\n",
    "index_list.remove(7)\n",
    "index_list = [str(i) for i in index_list]\n",
    "\n",
    "actual_prices = index_returns.loc[ : , index_list ]\n",
    "predicted_returns = index_predictions\n",
    "\n",
    "# print(actual_prices.describe())\n",
    "# print(predicted_returns.describe())\n",
    "\n",
    "# Load the variance data for each stock\n",
    "variances = pd.DataFrame(np.diag(covariance_matrix),index=index_list)\n",
    "\n",
    "def objective(weights, predicted_returns, variances):\n",
    "    \"\"\"\n",
    "    Compute the objective function for mean-variance optimization.\n",
    "    \"\"\"\n",
    "    mean_return = np.dot(weights.T, predicted_returns.mean(axis=0))\n",
    "    print(mean_return.shape)\n",
    "    variance = np.dot(weights.T, np.dot(np.diag(variances), weights))\n",
    "    print(variance.shape)\n",
    "    return -mean_return / np.sqrt(variance)\n",
    "\n",
    "def constraint(weights):\n",
    "    \"\"\"\n",
    "    Constraint function to ensure long and short positions net out completely.\n",
    "    \"\"\"\n",
    "    return np.sum(np.abs(weights)) - np.sum(weights)\n",
    "\n",
    "# Set up the optimization problem\n",
    "initial_weights = np.zeros(14)\n",
    "constraints = ({'type': 'eq', 'fun': constraint})\n",
    "bounds = [(-1, 1) for i in range(14)]\n",
    "\n",
    "# Solve for the optimal portfolio allocation\n",
    "result = minimize(objective, initial_weights, args=(predicted_returns, variances), constraints=constraints, bounds=bounds)\n",
    "weights = result.x\n",
    "\n",
    "# Compute the allocation for each stock\n",
    "positive_weights = weights[weights > 0]\n",
    "negative_weights = weights[weights < 0]\n",
    "total_weight = np.sum(np.abs(negative_weights))\n",
    "positive_allocation = positive_weights / np.sum(positive_weights)\n",
    "negative_allocation = np.abs(negative_weights) / total_weight\n",
    "allocation = np.zeros(14)\n",
    "allocation[weights > 0] = positive_allocation\n",
    "allocation[weights < 0] = -negative_allocation\n",
    "\n",
    "# Compute the position for each stock based on the allocation\n",
    "position = allocation * 1\n",
    "\n",
    "# Print the allocation and position for each stock\n",
    "for i in range(14):\n",
    "    print(f\"Stock {i+1}: Allocation={allocation[i]:.2f}, Position={position[i]:.2f}\")\n",
    "\n",
    "# In this example, we again load the actual stock prices and predicted stock returns from a CSV file and the variance data from a numpy array. We define the objective function and constraint function for mean-variance optimization. However, this time we define the constraint function differently to ensure that the long and short positions net out completely.\n",
    "\n",
    "# We set up the optimization problem and solve for the optimal portfolio allocation. However, since we want the long and short positions to net out completely, we need to adjust the allocation accordingly. We first separate the positive and negative weights, and compute the allocation separately for each. We then adjust the allocation for the negative weights to ensure that they net out completely with the positive weights. Finally, we compute the position for each stock based on the allocation.\n",
    "\n",
    "# Note that in this example, we assume that we can only long or short a maximum of 1 dollar for each timestep. If we want to trade more than 1 dollar, we need to adjust the position accordingly. We also assume that the variance data is known and fixed, which may not be the case in a real-world scenario.\n",
    "\n",
    "# Sure, here's an example of how to compute the mean, standard deviation, and Sharpe ratio of the trading strategy in Python:\n",
    "\n",
    "# Compute the returns for the trading strategy\n",
    "returns = np.dot(allocation, predicted_returns.T)\n",
    "\n",
    "# Compute the mean, standard deviation, and Sharpe ratio of the returns\n",
    "mean_return = np.mean(returns)\n",
    "stdev_return = np.std(returns)\n",
    "sharpe_ratio = mean_return / stdev_return\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Return: {mean_return:.4f}\")\n",
    "print(f\"Standard Deviation: {stdev_return:.4f}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
    "# In this example, we first compute the returns for the trading strategy by taking the dot product of the allocation and predicted returns. We then compute the mean, standard deviation, and Sharpe ratio of the returns using numpy functions. Finally, we print the results.\n",
    "\n",
    "# Note that the assumptions in the above execution scheme include:\n",
    "\n",
    "# We assume that the predicted stock returns are accurate and reliable. In reality, there may be errors or uncertainties in the predictions that could affect the performance of the trading strategy.\n",
    "# We assume that the variance data is known and fixed. In reality, the variances of the stock returns may change over time, which could affect the performance of the trading strategy.\n",
    "# We assume that we can only long or short a maximum of 1 dollar for each timestep. In reality, there may be constraints on the maximum position size or other trading rules that could affect the performance of the trading strategy.\n",
    "# We assume that the optimization problem is well-posed and can be solved accurately. In reality, the optimization problem may be ill-posed or difficult to solve, which could affect the performance of the trading strategy.\n",
    "# We assume that the past performance of the trading strategy is a good indicator of its future performance. In reality, the market conditions and other factors may change over time, which could affect the performance of the trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,)\n",
      "Stock 1: Allocation=0.06, Position=0.06\n",
      "Stock 2: Allocation=0.13, Position=0.13\n",
      "Stock 3: Allocation=0.06, Position=0.06\n",
      "Stock 4: Allocation=0.13, Position=0.13\n",
      "Stock 5: Allocation=0.10, Position=0.10\n",
      "Stock 6: Allocation=0.01, Position=0.01\n",
      "Stock 7: Allocation=0.01, Position=0.01\n",
      "Stock 8: Allocation=0.08, Position=0.08\n",
      "Stock 9: Allocation=0.01, Position=0.01\n",
      "Stock 10: Allocation=0.13, Position=0.13\n",
      "Stock 11: Allocation=0.06, Position=0.06\n",
      "Stock 12: Allocation=0.11, Position=0.11\n",
      "Stock 13: Allocation=0.03, Position=0.03\n",
      "Stock 14: Allocation=0.04, Position=0.04\n",
      "Stock 15: Allocation=0.04, Position=0.04\n",
      "[        nan -0.24341593  0.12322497 ... -0.18650551 -0.15469033\n",
      "  0.19121042]\n",
      "Mean Return: nan\n",
      "Standard Deviation: nan\n",
      "Sharpe Ratio: nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Extract the actual stock prices and predicted stock returns for each stock\n",
    "actual_prices = index_returns\n",
    "predicted_returns = index_predictions\n",
    "predicted_returns['7'] = index_returns['7']\n",
    "\n",
    "# print(predicted_returns.mean(axis=0).to_numpy())\n",
    "# print(initial_weights)\n",
    "\n",
    "# Load the variance data for each stock\n",
    "variances = np.array([0.1, 0.2, 0.15, 0.25, 0.12, 0.18, 0.08, 0.1, 0.2, 0.15, 0.25, 0.12, 0.18, 0.08, 0.1])\n",
    "\n",
    "def objective(weights, predicted_returns, variances):\n",
    "    \"\"\"\n",
    "    Compute the objective function for mean-variance optimization.\n",
    "    \"\"\"\n",
    "    mean_return = np.dot(weights.T, predicted_returns.mean(axis=0).to_numpy())\n",
    "    variance = np.dot(weights.T, variances*weights)\n",
    "    return -mean_return / np.sqrt(variance)\n",
    "\n",
    "def constraint(weights):\n",
    "    \"\"\"\n",
    "    Constraint function to ensure long and short positions net out completely.\n",
    "    \"\"\"\n",
    "    return np.sum(np.abs(weights)) - np.sum(weights)\n",
    "\n",
    "# Set up the optimization problem\n",
    "initial_weights = np.random.rand(15,1).reshape(15)\n",
    "constraints = ({'type': 'eq', 'fun': constraint})\n",
    "bounds = [(-1, 1) for i in range(15)]\n",
    "\n",
    "# Solve for the optimal portfolio allocation\n",
    "result = minimize(objective, initial_weights, args=(predicted_returns, variances), constraints=constraints, bounds=bounds)\n",
    "# print(result)\n",
    "weights = result.x\n",
    "print(weights.shape)\n",
    "\n",
    "# Compute the allocation for each stock\n",
    "positive_weights = weights[weights > 0]\n",
    "negative_weights = weights[weights < 0]\n",
    "total_weight = np.sum(np.abs(negative_weights))\n",
    "positive_allocation = positive_weights / np.sum(positive_weights)\n",
    "negative_allocation = np.abs(negative_weights) / total_weight\n",
    "allocation = np.zeros(15)\n",
    "allocation[weights > 0] = positive_allocation\n",
    "allocation[weights < 0] = -negative_allocation\n",
    "\n",
    "# Compute the position for each stock based on the allocation\n",
    "position = allocation * 1\n",
    "\n",
    "# Print the allocation and position for each stock\n",
    "for i in range(15):\n",
    "    print(f\"Stock {i+1}: Allocation={allocation[i]:.2f}, Position={position[i]:.2f}\")\n",
    "\n",
    "# print(returns.shape)\n",
    "# print(predicted_returns.T.shape)\n",
    "# Compute the returns for the trading strategy\n",
    "returns = np.dot(allocation, predicted_returns.T)\n",
    "print(returns)\n",
    "\n",
    "# Compute the mean, standard deviation, and Sharpe ratio of the returns\n",
    "mean_return = np.mean(returns)\n",
    "stdev_return = np.std(returns)\n",
    "sharpe_ratio = mean_return / stdev_return\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Return: {mean_return:.4f}\")\n",
    "print(f\"Standard Deviation: {stdev_return:.4f}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0         1         2         3         4         5         6  \\\n",
      "1      -3.813718 -0.482901  0.574910  2.270976  0.776114  0.575093  5.552967   \n",
      "2       2.162276 -0.404774  0.365852 -1.350740  0.882502  0.365968  1.184262   \n",
      "3       1.124595  0.006841 -0.522646  0.106836  1.289452  1.672997 -4.614627   \n",
      "4       4.613970 -0.632970 -0.522646 -2.304943 -3.415103 -0.522812 -6.014006   \n",
      "5       0.518727 -0.585448  0.365852  2.273576  0.161257 -0.522812  2.528405   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "199995  3.088871  0.751899 -1.045291 -3.333527 -0.917390 -1.672997  4.382245   \n",
      "199996 -0.767949 -1.533943  0.365852  1.785761  0.396643 -1.097904 -1.898434   \n",
      "199997 -1.047323  0.236460 -1.097556 -0.484082 -3.229254  0.575093  2.289020   \n",
      "199998 -1.423336 -0.571317 -1.672466 -6.772663 -2.447138  1.672997 -0.750087   \n",
      "199999 -5.267382 -0.874139  1.672466  0.210809 -3.950857 -1.672997  1.299305   \n",
      "\n",
      "               8         9        10        11        12        13        14  \n",
      "1       0.671358  2.517528  2.653193 -1.167151 -0.267749 -1.158363  1.673755  \n",
      "2       1.161958  1.592336 -3.152393  1.055175  0.228152  2.702177 -1.046097  \n",
      "3      -0.317244 -3.470978  1.431740 -0.520326 -0.409330 -2.001151 -0.523048  \n",
      "4      -1.309678  1.422233 -1.606689 -0.214022  0.192986  0.456047  1.046097  \n",
      "5       0.433183  2.667008 -0.436014 -0.209556  0.402888 -2.304332 -0.523048  \n",
      "...          ...       ...       ...       ...       ...       ...       ...  \n",
      "199995 -1.733432  1.970924  6.286848  0.551234  0.647850  1.039195  1.046097  \n",
      "199996 -0.393123  6.460209  0.974637 -0.349281 -6.516688  1.905604 -0.523048  \n",
      "199997  0.846031  4.044812 -5.790223 -1.080102  0.103636  1.905643  1.673755  \n",
      "199998 -0.093784  2.456556  1.796089  0.090033 -0.896430 -0.103510  1.673755  \n",
      "199999 -0.510884 -0.039361 -2.890965 -0.486037 -0.403240  2.301495 -0.523048  \n",
      "\n",
      "[199999 rows x 14 columns]\n",
      "               0         1         2         3         4         5         6  \\\n",
      "0      -0.447925  0.331247 -0.047379  0.830355 -0.403109  0.469433  0.669045   \n",
      "1       1.092257  0.200792  0.329483 -2.256335 -0.640973  0.303566  1.845568   \n",
      "2       1.425130 -0.183662  0.029004 -1.942503  1.542467  0.601067  0.947975   \n",
      "3       1.138974 -0.110281  0.208019  1.525294 -2.794868 -0.306181 -0.837877   \n",
      "4       0.861447  0.014652 -0.025191 -0.284543  0.613758 -0.392653  0.080076   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "199994  1.401351  0.260812 -0.403523  0.533750 -0.413131 -0.150885 -1.333865   \n",
      "199995  1.056444 -0.251221  0.210028  0.404529  0.735277 -0.155234 -1.044470   \n",
      "199996 -0.529844 -0.382821 -0.437382  0.144063 -1.081650 -0.124015 -1.141311   \n",
      "199997  0.495540 -0.349607 -0.084600  0.221688 -1.032409  0.452296 -0.749485   \n",
      "199998  0.345456 -0.090821  0.412650  0.299040  0.583433  0.386825  1.524265   \n",
      "\n",
      "               8         9        10        11        12        13        14  \n",
      "0       0.220209  0.457119  1.390502 -1.519834 -0.296371 -0.645486  0.678101  \n",
      "1       0.376925  0.545267 -1.035530  0.226350 -0.244801  0.813975  0.014249  \n",
      "2       0.243388  1.925136  0.407035  0.657543  0.538969 -0.068533 -0.049876  \n",
      "3      -0.577739  0.484897  0.259319  0.273074  1.877240 -0.092030  1.651237  \n",
      "4      -0.062552 -0.774026  2.592595 -0.044105 -0.255713 -0.663912 -0.027487  \n",
      "...          ...       ...       ...       ...       ...       ...       ...  \n",
      "199994 -0.364098  0.112641  0.556561  0.687316 -0.756318  0.343541  0.732389  \n",
      "199995  0.027806  4.066222  2.028087  0.121744 -1.540305  0.157390  0.201459  \n",
      "199996 -0.269262  1.577351  0.499937 -0.469116  0.029727  0.153100 -0.286433  \n",
      "199997  0.032664  0.615822 -0.327890 -0.367245  0.100829 -0.372675  0.606509  \n",
      "199998  0.369353 -1.379954  0.357446 -0.522019 -0.484026  0.656986  0.748872  \n",
      "\n",
      "[199999 rows x 14 columns]\n",
      "['0', '1', '2', '3', '4', '5', '6', '8', '9', '10', '11', '12', '13', '14']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "index_list = list(range(15))\n",
    "index_list.remove(7)\n",
    "index_list = [str(i) for i in index_list]\n",
    "\n",
    "# Extract the actual stock prices and predicted stock returns for each stock\n",
    "actual_prices = index_returns\n",
    "print(actual_prices)\n",
    "predicted_returns = index_predictions\n",
    "print(predicted_returns)\n",
    "print(index_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "a = np.array([2,3,4])\n",
    "b= np.array([2,3,4])\n",
    "print(np.dot(a.T,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           preds         y\n",
      "0      -0.047379  0.522361\n",
      "1       0.329483  0.333824\n",
      "2       0.029004 -0.467456\n",
      "3       0.208019 -0.467456\n",
      "4      -0.025191  0.333824\n",
      "...          ...       ...\n",
      "199994 -0.403523 -0.938797\n",
      "199995  0.210028  0.333824\n",
      "199996 -0.437382 -0.985931\n",
      "199997 -0.084600 -1.504406\n",
      "199998  0.412650  1.512177\n",
      "\n",
      "[199999 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# preds = model.predict(X)\n",
    "# score = pearsonr(y,preds)\n",
    "# print(score)\n",
    "\n",
    "for index in index_returns.columns:\n",
    "    index = '2'\n",
    "    preds = index_predictions[index]\n",
    "    y = norm_index_returns[index]\n",
    "    df = pd.DataFrame()\n",
    "    df['preds'] = preds\n",
    "    df['y'] = list(y)\n",
    "    print(df)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# model = tf.keras.models.Sequential()\n",
    "\n",
    "# model = keras.Sequential([\n",
    "#   keras.layers.Dense(units=15, activation='relu', input_shape=(25,)),\n",
    "#   keras.layers.Dense(units=5, activation='relu'),\n",
    "#   keras.layers.Dense(units=1, activation='linear')\n",
    "# ])\n",
    "\n",
    "# model.add(tf.keras.layers.)\n",
    "# # Building the input and the hidden layers\n",
    "# model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu))\n",
    "# model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu))\n",
    "# # Building the output layer\n",
    "# model.add(tf.keras.layers.Dense(1,activation = 'linear'))\n",
    "\n",
    "# model.compile(optimizer=\"adam\", loss=\"MeanSquaredError\", metrics=[\"accuracy\",'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ROG\\Desktop\\Limestone Data comp\\limestone-tower-research\\other.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X33sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m A \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(num_indices)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X33sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m b \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1.0\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X33sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(quadprog(P, q, G, h, A, b)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X33sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m weights[t, :], _, _ \u001b[39m=\u001b[39m quadprog(P, q, G, h, A, b)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Desktop/Limestone%20Data%20comp/limestone-tower-research/other.ipynb#X33sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# normalize the weights to ensure a maximum long/short position of $1\u001b[39;00m\n",
      "File \u001b[1;32mquadprog\\quadprog.pyx:12\u001b[0m, in \u001b[0;36mquadprog.solve_qp\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstringsource:660\u001b[0m, in \u001b[0;36mView.MemoryView.memoryview_cwrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstringsource:350\u001b[0m, in \u001b[0;36mView.MemoryView.memoryview.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'DataFrame'"
     ]
    }
   ],
   "source": [
    "from quadprog import solve_qp as quadprog\n",
    "\n",
    "# vector of stock prices\n",
    "stock_prices = index_returns.loc[: , index_list]\n",
    "\n",
    "# matrix of index predictions\n",
    "index_predictions = index_predictions\n",
    "\n",
    "# covariance matrix\n",
    "covariance = covariance_matrix\n",
    "\n",
    "# number of stocks and indices\n",
    "num_stocks = stock_prices.shape[1]\n",
    "num_indices = index_predictions.shape[1]\n",
    "\n",
    "# initialize portfolio weights to zero\n",
    "weights = np.zeros((num_indices, num_stocks))\n",
    "\n",
    "# iterate over each time step\n",
    "for t in range(1, len(stock_prices)):\n",
    "    # calculate the predicted returns and covariance for this time step\n",
    "    pred_returns = index_predictions.loc[[t], :]\n",
    "    cov = covariance\n",
    "\n",
    "    # use quadratic programming to find the optimal weights\n",
    "    P = cov\n",
    "    q = np.zeros(num_indices)\n",
    "    G = np.vstack([np.eye(num_indices), -np.eye(num_indices)])\n",
    "    h = np.ones(num_indices)\n",
    "    A = np.ones(num_indices)\n",
    "    b = np.array([1.0])\n",
    "    print(type(quadprog(P, q, G, h, A, b)))\n",
    "    weights[t, :], _, _ = quadprog(P, q, G, h, A, b)\n",
    "\n",
    "    # normalize the weights to ensure a maximum long/short position of $1\n",
    "    not_zero = weights[t, :] != 0\n",
    "    weights[t, not_zero] = weights[t, not_zero] / np.sum(np.abs(weights[t, not_zero]))\n",
    "\n",
    "    # calculate the trades for this time step\n",
    "    trades = weights[t, :] * stock_prices[t - 1, :]\n",
    "\n",
    "    # ensure that the long-short positions net out completely\n",
    "    trades = trades - np.mean(trades)\n",
    "\n",
    "    # update the portfolio value for this time step\n",
    "    portfolio_value = np.sum(trades)\n",
    "    \n",
    "mean_return = np.mean(np.sum(weights * index_predictions, axis=1))\n",
    "std_dev = np.std(np.sum(weights * index_predictions, axis=1))\n",
    "sharpe_ratio = mean_return/std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0 belongs to sector 0\n",
      "0.44327480635250455\n",
      "Index 1 could not be solved\n",
      "0.31119206523274157\n",
      "Index 2 could not be solved\n",
      "0.20610570135392597\n",
      "Index 3 could not be solved\n",
      "0.34199729775396315\n",
      "Index 4 belongs to sector 2\n",
      "0.44645989420224036\n",
      "Index 5 could not be solved\n",
      "0.21579737697124057\n",
      "Index 6 belongs to sector 3\n",
      "0.44380184920735266\n",
      "Index 7 could not be solved\n",
      "0.3018985083553388\n",
      "Index 8 could not be solved\n",
      "0.304073300302279\n",
      "Index 9 could not be solved\n",
      "0.34616522003027295\n",
      "Index 10 could not be solved\n",
      "0.3387910378670748\n",
      "Index 11 could not be solved\n",
      "0.002980653833538585\n",
      "Index 12 could not be solved\n",
      "0.008467730813550296\n",
      "Index 13 could not be solved\n",
      "0.005856225906110284\n",
      "Index 14 could not be solved\n",
      "0.2077396150139465\n"
     ]
    }
   ],
   "source": [
    "# previous method\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "import ppscore as pps\n",
    "\n",
    "# Choose a regressor: LinearRegression, Lasso, or Ridge\n",
    "\n",
    "regressor = LinearRegression(fit_intercept=True)\n",
    "index_predictions = pd.DataFrame()\n",
    "\n",
    "for index in index_returns.columns:\n",
    "    \n",
    "    best_score = -1\n",
    "    best_sector = None\n",
    "    best_preds = None\n",
    "    \n",
    "    for sector in sectors:\n",
    "        X = norm_stock_returns[sectors[sector]]\n",
    "        y = norm_index_returns[index]\n",
    "        # print(type(y))\n",
    "\n",
    "        model = regressor.fit(X.loc[0:10000,:], y[:10000])\n",
    "        preds = model.predict(X)\n",
    "\n",
    "        df = pd.DataFrame(columns=['y','preds'])\n",
    "        df['y'] = y\n",
    "        df['preds'] = preds\n",
    "\n",
    "        score = pearsonr(y, preds)\n",
    "        # score = np.correlation(preds,y)\n",
    "        score = score[0]\n",
    "        # score = r2_score(y, preds)\n",
    "        # score = score**0.5\n",
    "        # print(score)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_sector = sector\n",
    "            best_preds = preds\n",
    "            # print(score)\n",
    "\n",
    "    if best_score >= 0.4:  # 40% predictive correlation\n",
    "        print(f\"Index {index} belongs to sector {best_sector}\")\n",
    "        index_predictions[index] = best_preds\n",
    "        print(best_score)\n",
    "    else:\n",
    "        print(f\"Index {index} could not be solved\")\n",
    "        print(best_score)\n",
    "\n",
    "# Compute the covariance matrix of the index predictions\n",
    "covariance_matrix = index_predictions.cov()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
